{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "COMPSCI_4ML3_A3.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/EmilySanderson/EmilySanderson.github.io/blob/master/COMPSCI_4ML3_A3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_eGA5YxF97E3"
      },
      "source": [
        "# Application of ML in analyzing text documents\n",
        "In this asssignment we take advantage of scikit-learn in working with text documents. If you have missed the tutorial, you are encouraged to watch the associated tutorial. This will also be an excercise to figure out how to write a code with a new machine learning package; this is a necessary skill in applied machine learning, since the packages evolve quickly (and there are many of them) so being able to figure out how to work with a tool within a reasonble time frame is important. If you need further details you can check out to this <a href=\"https://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html\" > scikit example </a>, or other scikit documentations.\n",
        "\n",
        "# Submission\n",
        "- There are three tasks for you.\n",
        "- Report the results and answer the questions in a pdf file, along with your other solutions.\n",
        "- Additionally, submit your code in the same Jupiter notebook format. (keep the overal format of the notebook unchanged)\n",
        "\n",
        "Make a copy of this colab so that you can modify it for yourself. If google colab is slow, you can also download the notebook and use Jupyter notebook on your computer (just like assignment 2). Using the online notebook has the benefit that all the required packages are already installed.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hRjQi9nP-CdA"
      },
      "source": [
        "# Packages\n",
        "\n",
        "First of all, let's import the packages we need for this assignment.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xNRer4Oz-Mxd"
      },
      "source": [
        "# loading need libraries\n",
        "import numpy as np\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfTransformer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oQyhj3jX-Oqk"
      },
      "source": [
        "# Dataset characteristics\n",
        "\n",
        "Here we take a look at the structure/properties of the dataset. To have a faster code, we just pick 4 class labels out of 20 from this dataset. We are going to classify the documents into these 4 categories. So each data point is a text document.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UAPTE5Jj-RdK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "be645590-d8da-4029-b8c2-41556b97498f"
      },
      "source": [
        "categories = ['alt.atheism', 'soc.religion.christian',\n",
        "              'comp.graphics', 'sci.med']\n",
        "twenty_train = fetch_20newsgroups(subset='train',\n",
        "    categories=categories, shuffle=True, random_state=42)\n",
        "twenty_test = fetch_20newsgroups(subset='test',\n",
        "    categories=categories, shuffle=True, random_state=42)\n",
        "\n",
        "print(\"Dataset properties on train section:\")\n",
        "print(\"\\t Number of training data points: %d\" % len(twenty_train.data))\n",
        "print(\"\\t Number of test data points: %d\" % len(twenty_test.data))\n",
        "print(\"\\t Number of Classes: %d\" % len(categories))\n",
        "print(\"\\t Class names: \" ,(twenty_train.target_names))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading 20news dataset. This may take a few minutes.\n",
            "Downloading dataset from https://ndownloader.figshare.com/files/5975967 (14 MB)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Dataset properties on train section:\n",
            "\t Number of training data points: 2257\n",
            "\t Number of test data points: 1502\n",
            "\t Number of Classes: 4\n",
            "\t Class names:  ['alt.atheism', 'comp.graphics', 'sci.med', 'soc.religion.christian']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s3BTn8wybzH5"
      },
      "source": [
        "# A sample of dataset\n",
        "We can see the first instance/element of the training set like this,"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qV74xmOgcabl",
        "outputId": "df1776bc-b291-4da0-e83d-71b62e27e59e"
      },
      "source": [
        "print(\"\\n\".join(twenty_train.data[0].split(\"\\n\")))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "From: sd345@city.ac.uk (Michael Collier)\n",
            "Subject: Converting images to HP LaserJet III?\n",
            "Nntp-Posting-Host: hampton\n",
            "Organization: The City University\n",
            "Lines: 14\n",
            "\n",
            "Does anyone know of a good way (standard PC application/PD utility) to\n",
            "convert tif/img/tga files into LaserJet III format.  We would also like to\n",
            "do the same, converting to HPGL (HP plotter) files.\n",
            "\n",
            "Please email any response.\n",
            "\n",
            "Is this the correct group?\n",
            "\n",
            "Thanks in advance.  Michael.\n",
            "-- \n",
            "Michael Collier (Programmer)                 The Computer Unit,\n",
            "Email: M.P.Collier@uk.ac.city                The City University,\n",
            "Tel: 071 477-8000 x3769                      London,\n",
            "Fax: 071 477-8565                            EC1V 0HB.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XyRQrlhMccdq"
      },
      "source": [
        "the category name of the instance can be found as follows:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YQXuwz5ickAM",
        "outputId": "8d987ec2-e8a0-4ce7-d6b7-ead3edc228c5"
      },
      "source": [
        "print(twenty_train.target_names[twenty_train.target[0]])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "comp.graphics\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CTuJXmgQdkYK"
      },
      "source": [
        "To get the categries of a range of data, e.g., first 10 samples, we can do something like this:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nr-57YnMeIpp",
        "outputId": "72a0e7c6-f04c-47fb-bc1e-bcb049e985a8"
      },
      "source": [
        "for t in twenty_train.target[:10]:\n",
        "    print(twenty_train.target_names[t])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "comp.graphics\n",
            "comp.graphics\n",
            "soc.religion.christian\n",
            "soc.religion.christian\n",
            "soc.religion.christian\n",
            "soc.religion.christian\n",
            "soc.religion.christian\n",
            "sci.med\n",
            "sci.med\n",
            "sci.med\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9av39ipneTYl"
      },
      "source": [
        "# Feature extraction\n",
        "since our data is text, to run classification models on the dataset we will turn them into vectors with numerical features. Therefore, in this section, we extract features using the **Bag of Words** method. To this regard, \n",
        "\n",
        "\n",
        "*   Assign an integer ID to each word in the dataset (like a dictionary).\n",
        "*   For each data point ( document i), count the number of occurances of word w and put it in X[i,j] where i is the i'th document and j is the index of the word w in the dictionary.\n",
        "Thus, if we have e.g., 10000 data points and 100000 words in the dictionary, then X will be a 10,000 by 100,000 matrix, which is huge! The good news is that most elements of the matrix X are zero (not all the words are used in every document). Therefore, it is possible to (somehow) just store non-zero elements and save up a lot of memory. Fortunately, the library that we use supports using \"sparse\" data representations, meaning that it does not actually store all the zero-values.\n",
        "# Tokenizing with scikit-learn\n",
        "In the following part we extract whole words that have been used in the dataset and compute their occurance count in each document. This shows number of documents are **2257** and number of features (unique words in the whole set of documents) is **35788**.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jXpFn4Anh-bF",
        "outputId": "2d8214ea-6594-433d-9789-857a5b0f0aff"
      },
      "source": [
        "\n",
        "count_vect = CountVectorizer()\n",
        "X_train_counts = count_vect.fit_transform(twenty_train.data)\n",
        "X_train_counts.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2257, 35788)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FbizhQupjOuo"
      },
      "source": [
        "Up to here, we turned each document into an occurrence feature map (i.e., bag of words representation). But there is an issue with this solution: longer documents tend to have larger occurrence values. This is not ideal; for example, if we just repeat the same text twice, we don't expect the category of that document to change, but the occurance values will drastically change. Solution: we better normalize each document by dividing the occurrence values of each word by the total number of words in the document (*tf* normalization, where tf stands for term-frequency).\n",
        "\n",
        "Another issue is that we have some words that are so common that do not give much information (think of words like \"is\", \"the\", etc.). In order to reduce the effect of those words, one can use the *tf-idf* method, where on top of normalizing based on the length of the documents (*tf*), we also downscale weights for words that are presented in many documents (*idf* stands for inverse document frequency)\n",
        "\n",
        "If you are interested to know more about tf-idf, feel free to check out the wikipedia page. For this assignment, we will use *tf* and also *tf-idf* noramalization.\n",
        "\n",
        "The below application of ***TfidfTransformer*** is showed when idf is turned off. Evidently, we don't observe any changes in our feature dimension after performing **tf-idf** step."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "402y-_ZUleyh",
        "outputId": "81aeebe3-528c-44f5-9548-9366c69ddba7"
      },
      "source": [
        "tf_transformer = TfidfTransformer(use_idf=False).fit(X_train_counts)\n",
        "X_train_tf = tf_transformer.transform(X_train_counts)\n",
        "X_train_tf.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2257, 35788)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BTGDzuZzmhPo"
      },
      "source": [
        "# Document classification\n",
        "Support Vector Machines (SVMs) are one of the most common classifiers in practices. Here we train an SVM classifier on the transformed features, and try to classify two tiny documents using the trained classifier."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ryAWDeFonFWm"
      },
      "source": [
        "\n",
        "clf = SVC().fit(X_train_tf, twenty_train.target)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fkUuLpkQnp26",
        "outputId": "ab5c1031-f651-45c7-acd3-12f8a8955000"
      },
      "source": [
        "docs_new = ['God is love', 'OpenGL on the GPU is fast']\n",
        "X_new_counts = count_vect.transform(docs_new)\n",
        "X_new_tfidf = tf_transformer.transform(X_new_counts)\n",
        "\n",
        "predicted = clf.predict(X_new_tfidf)\n",
        "\n",
        "for doc, category in zip(docs_new, predicted):\n",
        "    print('%r => %s' % (doc, twenty_train.target_names[category]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "'God is love' => soc.religion.christian\n",
            "'OpenGL on the GPU is fast' => comp.graphics\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xf3fSX2fpPb1"
      },
      "source": [
        "# <font color=\"red\">Task1 </font>\n",
        "Given the numerical features we can train a classifier. In the following, a simple linear SVM is trained on tf features and verified on the test dataset.\n",
        "## Pipeline\n",
        "We can create a \"pipeline\" for performing a sequence of steps, namely first extracting the words and creating vectors, then using tf or tf-idf, and then training the classifier. This helps to make our code cleaner (and allows for more code optimization, etc.) We utilize a pipeline to do vectorizer -> transformer -> classifier\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SvfXQ0cfqDo1",
        "outputId": "b93fa099-f830-465c-a31a-92326aad0264"
      },
      "source": [
        "text_clf = Pipeline([\n",
        "      ('vect', CountVectorizer()),\n",
        "      ('tfidf', TfidfTransformer(use_idf=False)),\n",
        "      ('clf', SVC(kernel='linear')),\n",
        "  ])\n",
        "text_clf.fit(twenty_train.data, twenty_train.target)\n",
        "docs_test = twenty_test.data\n",
        "predicted = text_clf.predict(docs_test)\n",
        "print('linear kernel, accuracy:{}'.format(np.mean(predicted == twenty_test.target)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "linear kernel, accuracy:0.8808255659121171\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gMhwckdqrj_p"
      },
      "source": [
        "\n",
        "\n",
        "*  **A. (Kernel effect - 15 points)** Try RBF SVM (which is a version of SVM that uses Gaussian kernel) on the above example. Report the performance on three different gamma values on the test set: 0.70, 0.650, and 0.60 (see https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html)\n",
        ". Can you justify the results? (why the higher/lower gamma worked better?)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "47znPqTusfEp"
      },
      "source": [
        "# your code comes here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GY2q_ofZsiyz"
      },
      "source": [
        "*  **B. (idf importance - 15 points)** How would the results of part \"a\" change if we turn on *TfidfTransformer(use_idf=True)*? Report the results and justify them."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TTOaRmZmyTfo"
      },
      "source": [
        "# your code comes here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4bUgxZOXH1Bs"
      },
      "source": [
        "# <font color=\"red\">Task2-Confusion matrix</font>\n",
        "The confusion matrix is a k x k matrix where k is the number of classes. Computing the confusion matrix gives more detailed information than just computing the accuracy of a classifier. The element on the row i and column j of this matrix indicates the number of data points that were from class i but we classified them as class j.\n",
        "\n",
        "In scikit, the confusion matrix is a 2d array (https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html). \n",
        "Let's fix gamma = 0.7 and turn on the use_idf flag.\n",
        "\n",
        "**A. (15 points)** Report the confusion matrix on the test data. What is the most common mistake of the classifier? (for example, you can say that the data points from category xxxx were classified as category yyyy 200 times, which is more than any other pair of classes.)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RW2qbl1rKjhZ"
      },
      "source": [
        "# your code comes here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Hzr9F6B1oHb"
      },
      "source": [
        "# <font color=\"red\">Task3- The effect of n-gram</font>\n",
        "Let's say we have a document that contains \"apple watch\", and another document that contains \"I looked at my watch and had a bit of the apple\". The problem is that the bag-of-words representation will say that each of these documents have one occurances of the word apple and one occurance of the word watch; therefore, we lose the important fact that the combination word \"apple watch\" was present in the first document. To address this, it is possible to use \"n-grams\".\n",
        "\n",
        "Normally CountVectorizer assumes unigrams which means it just counts the word in each document. The idea of n-gram is to have the capability to also count sequences of n consecutive words. For example, if we use 2-grams, then \"apple watch\" will be considered as a single word (as well as things like \"I looked\", \"my watch\", \"watch and\", ...).\n",
        "\n",
        "\n",
        "In scikit, if we set ngram_rangetuple: (min_n,max_n) = (1,2) it counts both single words and also sequences of two words. Further details are available in <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html#sklearn.feature_extraction.text.CountVectorizer\" > this link </a>.\n",
        "\n",
        "taking the following setting,\n",
        "*   vectorizer = CountVectorizer(ngram_range=n_gram)\n",
        "*   tf_idf = TfidfTransformer()\n",
        "\n",
        "**A. (20 points)** Compare the accuracy of svm with RBF kernel (gamma=0.7) for different values of n_gram = (1,1),(1,2), and (2,2) on the test set. Which one works better? Justify the result. Also report the number of features (dimension of the input) for each of the three cases.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lPnFNXwl2AdY"
      },
      "source": [
        "# your code comes here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FQrxCahuAaNX"
      },
      "source": [
        "**B. Word or Character analyzer? (20 points)** Now that we are using n-grams, we can actually use n-characters rather than n-words. In this section we aim to investigate the feature space and classificaition performance by setting *analayzer='char'*. So, repeat the previous part with *CountVectorizer(ngram_range=n_gram,analyzer='char')* where n_gram in [(1,2),(1,3),(1,4)]. Which one of the three works better? Report test accuracies and justify the results. Also report the number of features."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "elkjQ4hJ_Jwh"
      },
      "source": [
        "# your code comes here"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}